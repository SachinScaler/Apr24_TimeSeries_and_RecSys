{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SachinScaler/Apr24_TimeSeries_and_RecSys/blob/main/Types_of_Distances.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CONTENT\n",
        "####Different distance measures used by machine learning algorithms:\n",
        "- Euclidean Distance\n",
        "- Manhattan Distance\n",
        "- Cosine Similarity\n",
        "- Hamming Distance\n",
        "- Minkowski Distance"
      ],
      "metadata": {
        "id": "j_8HV2Pj0ogw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In all the distances explained below $x_{1}$ and $x_{2}$ points are considered as vectors of dimension $d$"
      ],
      "metadata": {
        "id": "zEqc_nsVGaCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Euclidean Distance:\n",
        "<img src = https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/015/423/original/Screenshot_2022-09-30_at_3.32.07_PM.png?1664532217 height = 400 width = 400>\n",
        "\n",
        "- It's the most commonly used distance measure.\n",
        "- It can be defined as the length of segment joining two points.\n",
        "\n",
        "- The formula of Euclidean distance for two points of dimensionality d is:   \n",
        "> - Euclidean$(x_1,x_2) = ( ∑_{j=1}^{j=d} {{ (x _{1j} - x _{2j} )}^2) }^{\\frac{1}{2}} $\n",
        "\n",
        "- The formula is simply using pythagorean theorem on the cartesian coordinates to calculate the distance.\n",
        "\n",
        "**Properties**\n",
        "- It is very intuitive to use.\n",
        "- It is not scale invariant (One has to normalize the data before using this)\n",
        "- As the dimensionality of data increases, this becomes lesser useful due to curse of dimensionality."
      ],
      "metadata": {
        "id": "tHBpEYSW0s8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Manhattan Distance:\n",
        "<img src = https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/015/438/original/Screenshot_2022-09-30_at_5.36.14_PM.png?1664538989 height = 400 width = 400>\n",
        "\n",
        "- Its also called taxi-cab or city-block distance\n",
        "- Imagine vectors that describe objects on a uniform grid such as a chessboard.Now Manhattan distance refers to the distance between vectors if they can move right or left only. There is no diagonal moment involved while calculating this distance.\n",
        "- Manhattan $(x_1,x_2) = ∑_{j=1}^{j=d} {{ | x _{1j} - x _{2j} |} } $\n",
        "- When the data has discrete or binary attributes, manhattan distance is helpful because it takes only those values into account which are possible."
      ],
      "metadata": {
        "id": "BOjeB74PSbnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Cosine Similarity\n",
        "<img src = https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/015/440/original/Screenshot_2022-09-30_at_5.48.27_PM.png?1664539782 height = 400 width = 400>\n",
        "\n",
        "- Cosine Similarity is used to counteract the high dimensionality problem of euclidean distance\n",
        "- The cosine similarity is simply the cosine of the angle between two vectors. It is the inner product of the vectors if they were normalized to both have length one.\n",
        "- $D(x_{1}, x_{2})$ = $\\cos (\\theta ) =   \\dfrac {x_{1} \\cdot x_{2}} {\\left\\| x_{2}\\right\\| \\left\\| x_{2}\\right\\|} $\n",
        "- It is used in high dimensional data\n",
        "- It is very commonly used in text-analysis related tasks.\n",
        "- Cosine similarity doesn't take magnitude of vectors into account, just their direction is considered."
      ],
      "metadata": {
        "id": "aiSRzoNKVPA8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Hamming Distance:\n",
        "<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/015/442/original/Screenshot_2022-09-30_at_6.36.29_PM.png?1664542613 height = 400 width = 400>\n",
        "\n",
        "- It is calculated as the number of positions at which the values of coordinates are different in two vectors. For eg. in the image shown above its simply 2.\n",
        "- It is typically used to compare binary strings\n",
        "- It can also be used to calculate similarity of strings\n",
        "- It is difficult to use when two vectors are of different lengths\n",
        "- It doesn't take actual values into account\n",
        "- It is not usable when magnitude is important\n",
        "- It is used in error correction/detection when transferring data\n",
        "- It can be used to measure the distance between categorical variables"
      ],
      "metadata": {
        "id": "I7WI8dlZgPv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Minkowski Distance:\n",
        "<img src=https://d2beiqkhq929f0.cloudfront.net/public_assets/assets/000/015/443/original/Screenshot_2022-09-30_at_6.50.24_PM.png?1664543477 height = 400 width = 400>\n",
        "\n",
        "- It is a distance used in normed vector space, i.e. in a space where distances can be represented as vectors.\n",
        "- It is actually generalization of euclidean and manhattan distances.\n",
        "- Minkowski$(x_1,x_2) = ( ∑_{j=1}^{j=d} {{ (x _{1j} - x _{2j} )}^p) }^{\\frac{1}{p}} $, where p can be used to manipulate the distance metric.\n",
        "- Here if p=1 => Manhattan distance.\n",
        "- Here if p=2 => Euclidean distance.\n",
        "- Here if p=infinity => The distance will become the greatest of difference between two vectors along any coordinate dimension.\n",
        "- We can iterate over different values of p to get a distance which works best for our use case.\n"
      ],
      "metadata": {
        "id": "lmGQaXFKjDKo"
      }
    }
  ]
}